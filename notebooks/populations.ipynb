{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gen_regions.regions import Regions\n",
    "\n",
    "DATA_FOLDER = \"../data/population\"\n",
    "REGIONS_PATH = \"../data/regions.csv\"\n",
    "CITIES_15k_PATH = DATA_FOLDER + \"/cities15000.csv\"\n",
    "WORLDCITIES_PATH = DATA_FOLDER + \"/worldcities.csv\"\n",
    "INDIA500_PATH = DATA_FOLDER + \"/india_top500_cities_r2.csv\"\n",
    "STATES_INCL_CHINA_PATH = DATA_FOLDER + \"/population_incl_china_states.csv\"\n",
    "US_STATE_PATH = DATA_FOLDER + \"/us_state_populations.csv\"\n",
    "WORLD_PATH = DATA_FOLDER + \"/WorldPopulation.csv\"\n",
    "\n",
    "\n",
    "def format_df(data, do_print=False):\n",
    "    data = data.set_index('name')\n",
    "    data = data['population'].dropna()\n",
    "    \n",
    "    if do_print:\n",
    "        print(data.shape)\n",
    "        \n",
    "    # some cities are duplicated (more places on Earth with the same name)\n",
    "    data = data.groupby('name').max()\n",
    "    \n",
    "    if do_print:\n",
    "        print(data.shape)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def load_regions(filepath):\n",
    "    data_regions = Regions()\n",
    "    with open(filepath, 'rt') as f:\n",
    "        data_regions.read_csv(f)\n",
    "        \n",
    "    return data_regions\n",
    "\n",
    "\n",
    "def load_regions_df(filepath, kind=None):\n",
    "    data_regions = load_regions(filepath)\n",
    "    \n",
    "    if kind is None:\n",
    "        data_regions = [(x.name, x.pop) for x in data_regions.regions.values()]\n",
    "    else:\n",
    "        data_regions = [(x.name, x.pop) for x in data_regions.regions.values() if x.kind == kind]\n",
    "        \n",
    "    data_regions = pd.DataFrame(data_regions, columns=[\"name\", \"population\"])\n",
    "    \n",
    "    data_regions = data_regions.set_index(\"name\")\n",
    "    \n",
    "    return data_regions\n",
    "\n",
    "\n",
    "def load_cities15000(filepath):\n",
    "    data = pd.read_csv(filepath, encoding = \"ISO-8859-1\")\n",
    "    data['name'] = data['asciiname'].apply(str.lower)\n",
    "\n",
    "#     data['name_lower'] = data['name'].apply(str.lower)\n",
    "    # split alternate names to set of names\n",
    "#     data['alternatenames'] = data['alternatenames'].dropna().apply(\n",
    "#         lambda x: {y.lower() for y in x.split(\",\")}\n",
    "#     )\n",
    "    data = format_df(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_worldcities(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    data['name'] = data['city_ascii'].apply(str.lower)\n",
    "    \n",
    "    data = format_df(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_india500(filepath):\n",
    "    data = pd.read_csv(INDIA500_PATH)\n",
    "    data['name'] = data['name_of_city'].apply(str.lower)\n",
    "    data['population'] = data['population_total']\n",
    "    \n",
    "    data = format_df(data)\n",
    "\n",
    "    return data\n",
    "    \n",
    "def load_states_incl_china(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    \n",
    "    data[\"name\"] = data[\"Province/State\"].apply(str.lower)\n",
    "    data[\"population\"] = data[\"Population\"]\n",
    "    \n",
    "    data = data[data[\"name\"] != \"unknow\"]\n",
    "    \n",
    "    data = format_df(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_us_state(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    \n",
    "    data[\"name\"] = data[\"State\"].apply(str.lower)\n",
    "    data[\"population\"] = data[\"2018 Population\"]\n",
    "    \n",
    "    data = format_df(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_world(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    \n",
    "    data[\"name\"] = data[\"Country\"].apply(str.lower)\n",
    "    data[\"population\"] = data[\"2016\"]\n",
    "    \n",
    "    data = format_df(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# load Regions dataset\n",
    "\n",
    "# load all population datasets\n",
    "# do some custom preprocessing such that they can be easily merged\n",
    "# merge one by one and see how the number of missing cities is decreasing\n",
    "# write to regions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this template to quickly create new loader functions for specific files\n",
    "def template(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    \n",
    "    data = format_df(data, do_print=True)\n",
    "    data = format_df(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection(regions, dataset):\n",
    "    missing = set(regions[regions['population'].isna()].index)\n",
    "    new_data = set(dataset.index)\n",
    "    intersect = missing.intersection(new_data)\n",
    "    print(len(intersect))\n",
    "    return intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataset(regions, dataset):\n",
    "    print(\"rows total:\\t\\t\", regions.shape[0])\n",
    "    print(\"rows with pop before:\\t\", regions.dropna().shape[0])\n",
    "    regions['population'] = regions['population'].combine_first(dataset)\n",
    "    print(\"rows with pop after:\\t\", regions.dropna().shape[0])\n",
    "    return regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_regions(filepath, df):\n",
    "    regions = load_regions(filepath)\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    for reg, row in df.iterrows():\n",
    "        regions.regions[reg].pop = int(row['population'])\n",
    "        \n",
    "    with open(filepath, 'w') as f:\n",
    "        regions.write_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update state populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = load_regions_df(REGIONS_PATH, kind=\"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows total:\t\t 98\n",
      "rows with pop before:\t 0\n",
      "rows with pop after:\t 40\n"
     ]
    }
   ],
   "source": [
    "dataset = load_states_incl_china(STATES_INCL_CHINA_PATH)\n",
    "regions = merge_dataset(regions, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows total:\t\t 98\n",
      "rows with pop before:\t 40\n",
      "rows with pop after:\t 86\n"
     ]
    }
   ],
   "source": [
    "dataset = load_us_state(US_STATE_PATH)\n",
    "regions = merge_dataset(regions, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows total:\t\t 98\n",
      "rows with pop before:\t 86\n",
      "rows with pop after:\t 86\n"
     ]
    }
   ],
   "source": [
    "dataset = load_world(WORLD_PATH)\n",
    "regions = merge_dataset(regions, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "update_regions(REGIONS_PATH, regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update city populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = load_regions_df(REGIONS_PATH, kind=\"city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows total:\t\t 3225\n",
      "rows with pop before:\t 1918\n",
      "rows with pop after:\t 1979\n"
     ]
    }
   ],
   "source": [
    "dataset = load_worldcities(WORLDCITIES_PATH)\n",
    "regions = merge_dataset(regions, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows total:\t\t 3225\n",
      "rows with pop before:\t 1979\n",
      "rows with pop after:\t 2240\n"
     ]
    }
   ],
   "source": [
    "dataset = load_cities15000(CITIES_15k_PATH)\n",
    "regions = merge_dataset(regions, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows total:\t\t 3225\n",
      "rows with pop before:\t 2240\n",
      "rows with pop after:\t 2240\n"
     ]
    }
   ],
   "source": [
    "dataset = load_india500(INDIA500_PATH)\n",
    "regions = merge_dataset(regions, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "update_regions(REGIONS_PATH, regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
